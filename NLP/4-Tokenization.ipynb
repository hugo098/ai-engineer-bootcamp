{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc49492f-301f-4b0c-a397-703fca922e78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d71e1c-a074-4185-bbad-b1753339fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/hugorodriguez/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8bf662-5e07-4356-b0a9-71fd70c8a719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Her cat's name is Luna.\", \"Her dog's name is max\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"Her cat's name is Luna. Her dog's name is max\"\n",
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c05264-9ede-498d-9be4-ae75eabd8fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her', 'cat', \"'s\", 'name', 'is', 'Luna']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Her cat's name is Luna\"\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dcbed6-f119-4eed-a372-f1f70f2a543b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her',\n",
       " 'cat',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'Luna',\n",
       " 'and',\n",
       " 'her',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'max']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_2 = \"Her cat's name is Luna and her dog's name is max\"\n",
    "word_tokenize(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515a922-4481-45a1-bec2-792712d9afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hugorodriguez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'her cat name luna dog name max i am hugo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "sentence_2 = \"Her cat's name is Luna and her dog's name is max and I'm Hugo\"\n",
    "\n",
    "sentence_no_stopwords = \" \".join(\n",
    "    [word for word in sentence_2.split() if word not in en_stopwords]\n",
    ")\n",
    "\n",
    "\n",
    "sentence_2_fix = contractions.fix(sentence_no_stopwords)\n",
    "sentence_2_fix = re.sub(r\"'s\\b\", \"\", sentence_2_fix).lower()\n",
    "\n",
    "sentence_2_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3edc863d-35d0-42c2-8726-08c7041d09ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her', 'cat', 'name', 'luna', 'dog', 'name', 'max', 'i', 'am', 'hugo']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence_2_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57197fd9-997a-4485-9651-4f6836eba2e2",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58971f7-69f8-4d09-8cff-2b4510c51b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cbdc74-52d8-48d3-9eba-b16c6ecd9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45582e4-f8b7-4ce4-be42-1999e88bc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_tokens = [\"connecting\", \"connect\", \"connectivity\", \"connect\", \"connects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e362cff-05d5-4ed7-8135-5f430dd0cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting :  connect\n",
      "connect :  connect\n",
      "connectivity :  connect\n",
      "connect :  connect\n",
      "connects :  connect\n"
     ]
    }
   ],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, \": \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f8dc8-5eee-4a1c-88f1-45826eb8e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_tokens = [\"learned\", \"learning\", \"learn\", \"learns\", \"learner\", \"learners\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2ea53-91e8-45ca-a67f-68a6db14bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned :  learn\n",
      "learning :  learn\n",
      "learn :  learn\n",
      "learns :  learn\n",
      "learner :  learner\n",
      "learners :  learner\n"
     ]
    }
   ],
   "source": [
    "for t in learn_tokens:\n",
    "    print(t, \": \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544e4d1-2e4d-4ecd-a27a-30fc4e96e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_tokens = [\"likes\", \"better\", \"worse\", \"studies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa7a3-2be4-4fbf-a4f2-d62d056108c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes :  like\n",
      "better :  better\n",
      "worse :  wors\n",
      "studies :  studi\n"
     ]
    }
   ],
   "source": [
    "for t in likes_tokens:\n",
    "    print(t, \": \", ps.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93435fb0-f9a7-40c5-bc45-f2cb35fe7a1e",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa168d0-d140-4a60-8d7f-1b5319e5ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hugorodriguez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "494cb63f-45f1-4a62-be4b-a7c695f705f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27d57d-fb0e-4e96-baca-a866f1f4cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting :  connecting\n",
      "connect :  connect\n",
      "connectivity :  connectivity\n",
      "connect :  connect\n",
      "connects :  connects\n"
     ]
    }
   ],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, \": \", lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab3989-6c4c-4e1d-8f69-ab451454263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned :  learned\n",
      "learning :  learning\n",
      "learn :  learn\n",
      "learns :  learns\n",
      "learner :  learner\n",
      "learners :  learner\n"
     ]
    }
   ],
   "source": [
    "for t in learn_tokens:\n",
    "    print(t, \": \", lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772d283-d64e-4a4b-9c65-3c8a7fad67ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes :  like\n",
      "better :  better\n",
      "worse :  worse\n",
      "studies :  study\n"
     ]
    }
   ],
   "source": [
    "for t in likes_tokens:\n",
    "    print(t, \": \", lemmatizer.lemmatize(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_course_env]",
   "language": "python",
   "name": "conda-env-nlp_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
